{"context": "What is Data Science?\nHello and welcome to the Data Scientist's Toolbox, the first course in the Data Science Specialization series. Here, we will be going over the basics of data science and introducing you to the tools that will be used throughout the series. So, the first question you probably need answered going into this course is, what is data science? That is a great question. To different people this means different things, but at its core, data science is using data to answer questions. This is a pretty broad definition and that's because it's a pretty broad field. Data science can involve statistics, computer science, mathematics, data cleaning and formatting, and data visualization. An Economist Special Report sums up this melange of skills well. They state that a data scientist is broadly defined as someone who combines the skills of software programmer, statistician, and storyteller/artists to extract the nuggets of gold hidden under mountains of data. By the end of these courses, hopefully you will feel equipped to do just that. One of the reasons for the rise of data science in recent years is the vast amount of data currently available and being generated. Not only are massive amounts of data being collected about many aspects of the world and our lives, but we simultaneously have the rise of inexpensive computing. This has created the perfect storm in which we enrich data and the tools to analyze it, rising computer memory capabilities, better processors, more software and now, more data scientists with the skills to put this to use and answer questions using this data. There is a little anecdote that describes the truly exponential growth of data generation we are experiencing. In the third century BC, the Library of Alexandria was believed to house the sum of human knowledge. Today, there is enough information in the world to give every person alive 320 times as much of it as historians think was stored in Alexandria's entire collection, and that is still growing. We'll talk a little bit more about big data in a later lecture. But it deserves an introduction here since it has been so integral to the rise of data science. There are a few qualities that characterize big data. The first is volume. As the name implies, big data involves large datasets. These large datasets are becoming more and more routine. For example, say you had a question about online video. Well, YouTube has approximately 300 hours of video uploaded every minute. You would definitely have a lot of data available to you to analyze. But you can see how this might be a difficult problem to wrangle all of that data. This brings us to the second quality of Big Data, velocity. Data is being generated and collected faster than ever before. In our YouTube example, new data is coming at you every minute. In a completely different example, say you have a question about shipping times of rats. Well, most transport trucks have real-time GPS data available. You could in real time analyze the trucks movements if you have the tools and skills to do so. The third quality of big data is variety. In the examples I've mentioned so far, you have different types of data available to you. In the YouTube example, you could be analyzing video or audio, which is a very unstructured dataset, or you could have a database of video lengths, views or comments, which is a much more structured data set to analyze. So, we've talked about what data science is and what sorts of data it deals with, but something else we need to discuss is what exactly a data scientist is. The most basic of definitions would be that a data scientist is somebody who uses data to answer questions. But more importantly to you, what skills does a data scientist embody? To answer this, we have this illustrative Venn diagram in which data science is the intersection of three sectors, substantive expertise, hacking skills, and math and statistics. To explain a little on what we mean by this, we know that we use data science to answer questions. So first, we need to have enough expertise in the area that we want to ask about in order to formulate our questions, and to know what sorts of data are appropriate to answer that question. Once we have our question and appropriate data, we know from the sorts of data that data science works with. Oftentimes it needs to undergo significant cleaning and formatting. This often takes computer programming/hacking skills. Finally, once we have our data, we need to analyze it. This often takes math and stats knowledge. In this specialization, we'll spend a bit of time focusing on each of these three sectors. But we'll primarily focus on math and statistics knowledge and hacking skills. For hacking skills, we'll focus on teaching two different components, computer programming or at least computer programming with R which will allow you to access data, play around with it, analyze it, and plot it. Additionally, we'll focus on having you learn how to go out and get answers to your programming questions. One reason data scientists are in such demand is that most of the answers are not already outlined in textbooks. A data scientist needs to be somebody who knows how to find answers to novel problems. Speaking of that demand, there is a huge need for individuals with data science skills. Not only are machine-learning engineers, data scientists, and big data engineers among the top emerging jobs in 2017 according to LinkedIn, the demand far exceeds the supply. They state, \"Data scientists roles have grown over 650 percent since 2012. But currently, 35,000 people in the US have data science skills while hundreds of companies are hiring for those roles - even those you may not expect in sectors like retail and finance. Supply of candidates for these roles cannot keep up with demand.\" This is a great time to be getting into data science. Not only do we have more and more data, and more and more tools for collecting, storing, and analyzing it, but the demand for data scientists is becoming increasingly recognized as important in many diverse sectors, not just business and academia. Additionally, according to Glassdoor, in which they ranked the top 50 best jobs in America, data scientist is THE top job in the US in 2017, based on job satisfaction, salary, and demand. The diversity of sectors in which data science is being used is exemplified by looking at examples of data scientists. One place we might not immediately recognize the demand for data science is in sports. Daryl Morey is the general manager of a US basketball team, the Houston Rockets. Despite not having a strong background in basketball, Morey was awarded the job as GM on the basis of his bachelor's degree in computer science and his MBA from MIT. He was chosen for his ability to collect and analyze data and use that to make informed hiring decisions. Another data scientists that you may have heard of his Hilary Mason. She is a co-founder of FastForward Labs, a machine learning company recently acquired by Cloudera, a data science company, and is the Data Scientist in Residence at Accel. Broadly, she uses data to answer questions about mining the web and understanding the way that humans interact with each other through social media. Finally, Nate Silver is one of the most famous data scientists or statisticians in the world today. He is founder and editor in chief at FiveThirtyEight, a website that uses statistical analysis - hard numbers - to tell compelling stories about elections, politics, sports, science, economics, and lifestyle. He uses large amounts of totally free public data to make predictions about a variety of topics. Most notably, he makes predictions about who will win elections in the United States, and has a remarkable track record for accuracy doing so. One great example of data science in action is from 2009 in which researchers at Google analyzed 50 million commonly searched terms over a five-year period and compared them against CDC data on flu outbreaks. Their goal was to see if certain searches coincided with outbreaks of the flu. One of the benefits of data science and using big data is that it can identify correlations. In this case, they identified 45 words that had a strong correlation with the CDC flu outbreak data. With this data, they have been able to predict flu outbreaks based solely off of common Google searches. Without this mass amounts of data, these 45 words could not have been predicted beforehand. Now that you have had this introduction into data science, all that really remains to cover here is a summary of what it is that we will be teaching you throughout this course. To start, we'll go over the basics of R. R is the main programming language that we will be working with in this course track. So, a solid understanding of what it is, how it works, and getting it installed on your computer is a must. We'll then transition into RStudio, which is a very nice graphical interface to R, that should make your life easier. We'll then talk about version control, why it is important, and how to integrate it into your work. Once you have all of these basics down, you'll be all set to apply these tools to answering your very own data science questions. Looking forward to learning with you. Let's get to it.\n\nWhat is Data?\nSince we've spent some time discussing what data science is, we should spend some time looking at what exactly data is. First, let's look at what a few trusted sources consider data to be. First up, we'll look at the Cambridge English Dictionary which states that data is information, especially facts or numbers collected to be examined and considered and used to help decision-making. Second, we'll look at the definition provided by Wikipedia which is, a set of values of qualitative or quantitative variables. These are slightly different definitions and they get a different components of what data is. Both agree that data is values or numbers or facts. But the Cambridge definition focuses on the actions that surround data. Data is collected, examined and most importantly, used to inform decisions. We've focused on this aspect before. We've talked about how the most important part of data science is the question and how all we are doing is using data to answer the question. The Cambridge definition focuses on this. The Wikipedia definition focuses more on what data entails. And although it is a fairly short definition, we'll take a second to parse this and focus on each component individually. So, the first thing to focus on is, a set of values. To have data, you need a set of items to measure from. In statistics, this set of items is often called the population. The set as a whole is what you are trying to discover something about. The next thing to focus on is, variables. Variables are measurements or characteristics of an item. Finally, we have both qualitative and quantitative variables. Qualitative variables are, unsurprisingly, information about qualities. They are things like country of origin, sex or treatment group. They're usually described by words, not numbers and they are not necessarily ordered. Quantitative variables on the other hand, are information about quantities. Quantitative measurements are usually described by numbers and are measured on a continuous ordered scale. They're things like height, weight and blood pressure. So, taking this whole definition into consideration we have measurements, either qualitative or quantitative on a set of items making up data. Not a bad definition. When we were going over the definitions, our examples of data, country of origin, sex, height, weight are pretty basic examples. You can easily envision them in a nice-looking spreadsheet like this one, with individuals along one side of the table in rows, and the measurements for those variables along the columns. Unfortunately, this is rarely how data is presented to you. The data sets we commonly encounter are much messier. It is our job to extract the information we want, corralled into something tidy like the table here, analyze it appropriately and often, visualize our results. These are just some of the data sources you might encounter. And we'll briefly look at what a few of these data sets often look like, or how they can be interpreted. But one thing they have in common is the messiness of the data. You have to work to extract the information you need to answer your question. One type of data that I work with regularly, is sequencing data. This data is generally first encountered in the fast queue format. The raw file format produced by sequencing machines. These files are often hundreds of millions of lines long, and it is our job to parse this into an understandable and interpretable format, and infer something about that individual's genome. In this case, this data was interpreted into expression data, and produced a plot called the Volcano Plot. One rich source of information is countrywide censuses. In these, almost all members of a country answer a set of standardized questions and submit these answers to the government. When you have that many respondents, the data is large and messy. But once this large database is ready to be queried, the answers embedded are important. Here we have a very basic result of the last US Census. In which all respondents are divided by sex and age. This distribution is plotted in this population pyramid plot. I urge you to check out your home country census bureau, if available and look at some of the data there. This is a mock example of an electronic medical record. This is a popular way to store health information, and more and more population-based studies are using this data to answer questions and make inferences about populations at large, or as a method to identify ways to improve medical care. For example, if you are asking about a population's common allergies, you will have to extract many individuals allergy information, and put that into an easily interpretable table format where you will then perform your analysis. A more complex data source to analyze our images slash videos. There is a wealth of information coded in an image or video, and it is just waiting to be extracted. An example of image analysis that you may be familiar with is when you upload a picture to Facebook. Not only does it automatically recognize faces in the picture, but then suggests who they maybe. A fun example you can play with is The Deep Dream software that was originally designed to detect faces in an image, but has since moved onto more artistic pursuits. There is another fun Google initiative involving image analysis, where you help provide data to Google's machine learning algorithm by doodling. Recognizing that we've spent a lot of time going over what data is, we need to reiterate data is important, but it is secondary to your question. A good data scientist asks questions first and seeks out relevant data second. Admittedly, often the data available will limit, or perhaps even enable certain questions you are trying to ask. In these cases, you may have to re-frame your question or answer a related question but the data itself does not drive the question asking. In this lesson we focused on data, both in defining it and in exploring what data may look like and how it can be used. First, we looked at two definitions of data. One that focuses on the actions surrounding data, and another on what comprises data. The second definition embeds the concepts of populations, variables and looks at the differences between quantitative and qualitative data. Second, we examined different sources of data that you may encounter and emphasized the lack of tidy data sets. Examples of messy data sets where raw data needs to be rankled into an interpretable form, can include sequencing data, census data, electronic medical records et cetera. Finally, we return to our beliefs on the relationship between data and your question and emphasize the importance of question first strategies. You could have all the data you could ever hope for, but if you don't have a question to start, the data is useless.\n\nThe Data Science Process\nIn the first few lessons of this course, we discuss what data and data science are and ways to get help. What we haven't yet covered is what an actual data science project looks like. To do so, we'll first step through an actual data science project, breaking down the parts of a typical project and then provide a number of links to other interesting data science projects. Our goal in this lesson is to expose you to the process one goes through as they carry out data science projects. Every data science project starts with a question that is to be answered with data. That means that forming the question is an important first step in the process. The second step, is finding or generating the data you're going to use to answer that question. With the question solidified and data in hand, the data are then analyzed first by exploring the data and then often by modeling the data, which means using some statistical or machine-learning techniques to analyze the data and answer your question. After drawing conclusions from this analysis, the project has to be communicated to others. Sometimes this is the report you send to your boss or team at work, other times it's a blog post. Often it's a presentation to a group of colleagues. Regardless, a data science project almost always involve some form of communication of the project's findings. We'll walk through these steps using a data science project example below. For this example, we're going to use an example analysis from a data scientist named Hilary Parker. Her work can be found on her blog and the specific project we'll be working through here is from 2013 entitled, Hilary: The most poison baby name in US history. To get the most out of this lesson, click on that link and read through Hilary's post. Once you're done, come on back to this lesson and read through the breakdown of this post. When setting out on a data science project, it's always great to have your question well-defined. Additional questions may pop up as you do the analysis. But knowing what you want to answer with your analysis is a really important first step. Hilary Parker's question is included in bold in her post. Highlighting this makes it clear that she's interested and answer the following question; is Hilary/Hillary really the most rapidly poison naming recorded American history? To answer this question, Hilary collected data from the Social Security website. This data set included 1,000 most popular baby names from 1880 until 2011. As explained in the blog post, Hilary was interested in calculating the relative risk for each of the 4,110 different names in her data set from one year to the next, from 1880-2011. By hand, this would be a nightmare. Thankfully, by writing code in R, all of which is available on GitHub, Hilary was able to generate these values for all these names across all these years. It's not important at this point in time to fully understand what a relative risk calculation is. Although, Hilary does a great job breaking it down in her post. But it is important to know that after getting the data together, the next step is figuring out what you need to do with that data in order to answer your question. For Hilary's question, calculating the relative risk for each name from one year to the next from 1880-2011, and looking at the percentage of babies named each name in a particular year would be what she needed to do to answer her question. What you don't see in the blog post is all of the code Hilary wrote to get the data from the Social Security website, to get it in the format she needed to do the analysis and to generate the figures. As mentioned above, she made all this code available on GitHub so that others could see what she did and repeat her steps if they wanted. In addition to this code, data science projects often involve writing a lot of code and generating a lot of figures that aren't included in your final results. This is part of the data science process to figuring out how to do what you want to do to answer your question of interest. It's part of the process. It doesn't always show up in your final project and can be very time consuming. That said, given that Hilary now had the necessary values calculated, she began to analyze the data. The first thing she did was look at the names with the biggest drop in percentage from one year to the next. By this preliminary analysis, Hilary was sixth on the list. Meaning there were five other names that had had a single year drop in popularity larger than the one the name Hilary experienced from 1992-1993. In looking at the results of this analysis, the first five years appeared peculiar to Hilary Parker. It's always good to consider whether or not the results were what you were expecting from many analysis. None of them seemed to be names that were popular for long periods of time. To see if this hunch was true, Hilary plotted the percent of babies born each year with each of the names from this table. What she found was that among these poisoned names, names that experienced a big drop from one year to the next in popularity, all of the names other than Hilary became popular all of a sudden and then dropped off in popularity. Hilary Parker was able to figure out why most of these other names became popular. So definitely read that section of her post. The name, Hilary, however, was different. It was popular for a while and then completely dropped off in popularity. To figure out what was specifically going on with the name Hilary, she removed names that became popular for short periods of time before dropping off and only looked at names that were in the top 1,000 for more than 20 years. The results from this analysis definitively showed that Hilary had the quickest fall from popularity in 1992 of any female baby named between 1880 and 2011. Marian's decline was gradual over many years. For the final step in this data analysis process, once Hilary Parker had answered her question, it was time to share it with the world. An important part of any data science project is effectively communicating the results of the project. Hilary did so by writing a wonderful blog post that communicated the results of her analysis. Answered the question she set out to answer, and did so in an entertaining way. Additionally, it's important to note that most projects build off someone else's work. It's really important to give those people credit. Hilary accomplishes this by linking to a blog post where someone had asked a similar question previously, to the Social Security website where she got the data and where she learned about web scraping. Hilary's work was carried out using the R programming language. Throughout the courses in this series, you'll learn the basics of programming in R, exploring and analyzing data, and how to build reports and web applications that allow you to effectively communicate your results. To give you an example of the types of things that can be built using the R programming and suite of available tools that use R, below are a few examples of the types of things that have been built using the data science process and the R programming language. The types of things that you'll be able to generate by the end of this series of courses. Masters students at the University of Pennsylvania set out to predict the risk of opioid overdoses in Providence, Rhode Island. They include details on the data they used. The steps they took to clean their data, their visualization process, and their final results. While the details aren't important now, seeing the process and what types of reports can be generated is important. Additionally, they've created a Shiny app, which is an interactive web application. This means that you can choose what neighborhood in Providence you want to focus on. All of this was built using R programming. The following are smaller projects than the example above, but data science projects nonetheless. In each project, the author had a question they wanted to answer and use data to answer that question. They explored, visualized, and analyzed the data. Then, they wrote blog posts to communicate their findings. Take a look to learn more about the topics listed and to see how others work through the data science project process and communicate their results. Maelle Samuel looked to use data to see where one should live in the US given their weather preferences. David Robinson carried out an analysis of Trump's tweets to show that Trump only writes the angrier ones himself. Charlotte Galvin used open data available from the City of Toronto to build a map with information about sexual health clinics. In this lesson, we hope we've conveyed that sometimes data science projects are tackling difficult questions. Can we predict the risk of opioid overdose? While other times the goal of the project is to answer a question you're interested in personally; is Hilary the most rapidly poisoned baby name in recorded American history? In either case, the process is similar. You have to form your question, get data, explore and analyze your data, and communicate your results. With the tools you will learn in this series of courses, you will be able to set out and carry out your own data science projects like the examples included in this lesson.\n", "context_len": 4939, "questions": ["Question 1. What is the main goal of data science?\nA. Analyze and predict future trends\nB. Generate massive amounts of data\nC. Answer questions using data\nD. Increase the use of technology", "Question 2. The rise of data science is largely due to:\nA. Reduction in data generation\nB. Rapid increase in computing capabilities and data generation\nC. Increase in computer programming skills\nD. Rise in demand for statisticians", "Question 3. Which of the following characteristics does not belong to big data?\nA. Volume\nB. Velocity\nC. Variety\nD. Valuation", "Question 4. Which of the following represents 'qualitative' data?\nA. The weight of a person\nB. The gender of a person\nC. The age of a person\nD. The treatment group", "Question 5. Why do we need version control in data science?\nA. It allows you to revisit and compare different versions of your work.\nB. It increases the volume of data.\nC. It speeds up data analysis.\nD. It makes the data look visually appealing.", "Question 6. What does the 'volume' characteristic in Big Data refer to?\nA. The speed at which data is generated\nB. The different types of data\nC. The size of the datasets\nD. The volume of data from website or book", "Question 7. In the Venn diagram illustrating the data science field, which component is NOT included?\nA. Software programming\nB. Substantive expertise\nC. Data visualization\nD. Math and statistics", "Question 8. What do hacking skills in data science primarily pertain to?\nA. Gaining unauthorized access to data\nB. Data cleaning and formatting\nC. Performing illegal data cleaning and collection\nD. hack into others' computer system", "Question 9. Which of these is NOT a typical component of a data science project?\nA. Developing the question\nB. Gathering and preparing the data\nC. Running a focus group\nD. Communicating your findings\n", "Question 10. In the data science process, what typically follows data analysis?\nA. Asking a question\nB. Gathering data\nC. Make a presentation to show your findings\nD. report your findings", "Question 11. Which of the following best describes the concept of modeling in data science?\nA. Creating a physical model of the data\nB. Using statistical or machine-learning techniques to analyze the data\nC. Designing the layout of the data in a database\nD. Modeling the data distribution with deep learning", "Question 12. What can influence the type of questions you can ask in a data science project?\nA. The amount of data you have\nB. The type of data available to you\nC. The software you are using for data analysis\nD. People you want to interview.", "Question 13. Which of the following is NOT a standard step in a data science project?\nA. Gathering data\nB. Analyzing data\nC. Painting data\nD. communicating the results", "Question 14. In Hilary Parker's study of baby names, what unique characteristic did the name \"Hilary\" demonstrate when compared to other names that also dropped in popularity?\nA. The name Hilary rose in popularity suddenly and then dropped off.\nB. The name Hilary remained popular for an extended period and then experienced a significant drop in popularity.\nC. The name Hilary's popularity fluctuated frequently over the years.\nD. Hilary remained popular for a longer period."], "num_question": 14, "answer": ["C", "B", "D", "BD", "A", "C", "C", "B", "C", "CD", "B", "ABC", "C", "B"], "dataset": "coursera"}
{"context": "Mini-batch Gradient Descent\nHello, and welcome back. In this week, you learn about optimization algorithms that will enable you to train your neural network much faster. You've heard me say before that applying machine learning is a highly empirical process, is a highly iterative process. In which you just had to train a lot of models to find one that works really well. So, it really helps to really train models quickly. One thing that makes it more difficult is that Deep Learning tends to work best in the regime of big data. We are able to train neural networks on a huge data set and training on a large data set is just slow. So, what you find is that having fast optimization algorithms, having good optimization algorithms can really speed up the efficiency of you and your team. So, let's get started by talking about mini-batch gradient descent. You've learned previously that vectorization allows you to efficiently compute on all m examples, that allows you to process your whole training set without an explicit For loop. That's why we would take our training examples and stack them into these huge matrix capsule Xs. X1, X2, X3, and then eventually it goes up to XM training samples. And similarly for Y this is Y1 and Y2, Y3 and so on up to YM. So, the dimension of X was an X by M and this was 1 by M. Vectorization allows you to process all M examples relatively quickly if M is very large then it can still be slow. For example what if M was 5 million or 50 million or even bigger. With the implementation of gradient descent on your whole training set, what you have to do is, you have to process your entire training set before you take one little step of gradient descent. And then you have to process your entire training sets of five million training samples again before you take another little step of gradient descent. So, it turns out that you can get a faster algorithm if you let gradient descent start to make some progress even before you finish processing your entire, your giant training sets of 5 million examples. In particular, here's what you can do. Let's say that you split up your training set into smaller, little baby training sets and these baby training sets are called mini-batches. And let's say each of your baby training sets have just 1,000 examples each. So, you take X1 through X1,000 and you call that your first little baby training set, also call the mini-batch. And then you take home the next 1,000 examples. X1,001 through X2,000 and the next X1,000 examples and come next one and so on. I'm going to introduce a new notation. I'm going to call this X superscript with curly braces, 1 and I am going to call this, X superscript with curly braces, 2. Now, if you have 5 million training samples total and each of these little mini batches has a thousand examples, that means you have 5,000 of these because you know, 5,000 times 1,000 equals 5 million. Altogether you would have 5,000 of these mini batches. So it ends with X superscript curly braces 5,000 and then similarly you do the same thing for Y. You would also split up your training data for Y accordingly. So, call that Y1 then this is Y1,001 through Y2,000. This is called, Y2 and so on until you have Y5,000. Now, mini batch number T is going to be comprised of XT, and YT. And that is a thousand training samples with the corresponding input output pairs. Before moving on, just to make sure my notation is clear, we have previously used superscript round brackets I to index in the training set so X I, is the I-th training sample. We use superscript, square brackets L to index into the different layers of the neural network. So, ZL comes from the Z value, for the L layer of the neural network and here we are introducing the curly brackets T to index into different mini batches. So, you have XT, YT. And to check your understanding of these, what is the dimension of XT and YT? Well, X is an X by M. So, if X1 is a thousand training examples or the X values for a thousand examples, then this dimension should be Nx by 1,000 and X2 should also be Nx by 1,000 and so on. So, all of these should have dimension MX by 1,000 and these should have dimension 1 by 1,000. To explain the name of this algorithm, batch gradient descent, refers to the gradient descent algorithm we have been talking about previously. Where you process your entire training set all at the same time. And the name comes from viewing that as processing your entire batch of training samples all at the same time. I know it's not a great name but that's just what it's called. Mini-batch gradient descent in contrast, refers to algorithm which we'll talk about on the next slide and which you process is single mini batch XT, YT at the same time rather than processing your entire training set XY the same time. So, let's see how mini-batch gradient descent works. To run mini-batch gradient descent on your training sets you run for T equals 1 to 5,000 because we had 5,000 mini batches as high as 1,000 each. What are you going to do inside the For loop is basically implement one step of gradient descent using XT comma YT. It is as if you had a training set of size 1,000 examples and it was as if you were to implement the algorithm you are already familiar with, but just on this little training set size of M equals 1,000. Rather than having an explicit For loop over all 1,000 examples, you would use vectorization to process all 1,000 examples sort of all at the same time. Let us write this out. First, you implement forward prop on the inputs. So just on XT. And you do that by implementing Z1 equals W1. Previously, we would just have X there, right? But now you are processing the entire training set, you are just processing the first mini-batch so that it becomes XT when you're processing mini-batch T. Then you will have A1 equals G1 of Z1, a capital Z since this is actually a vectorized implementation and so on until you end up with AL, as I guess GL of ZL, and then this is your prediction. And you notice that here you should use a vectorized implementation. It's just that this vectorized implementation processes 1,000 examples at a time rather than 5 million examples. Next you compute the cost function J which I'm going to write as one over 1,000 since here 1,000 is the size of your little training set. Sum from I equals one through L of really the loss of Y^I YI. And this notation, for clarity, refers to examples from the mini batch XT YT. And if you're using regularization, you can also have this regularization term. Move it to the denominator times sum of L, Frobenius norm of the weight matrix squared. Because this is really the cost on just one mini-batch, I'm going to index as cost J with a superscript T in curly braces. You notice that everything we are doing is exactly the same as when we were previously implementing gradient descent except that instead of doing it on XY, you're not doing it on XT YT. Next, you implement back prop to compute gradients with respect to JT, you are still using only XT YT and then you update the weights W, really WL, gets updated as WL minus alpha D WL and similarly for B. This is one pass through your training set using mini-batch gradient descent. The code I have written down here is also called doing one epoch of training and epoch is a word that means a single pass through the training set. Whereas with batch gradient descent, a single pass through the training set allows you to take only one gradient descent step. With mini-batch gradient descent, a single pass through the training set, that is one epoch, allows you to take 5,000 gradient descent steps. Now of course you want to take multiple passes through the training set which you usually want to, you might want another for loop for another while loop out there. So you keep taking passes through the training set until hopefully you converge or at least approximately converged. When you have a large training set, mini-batch gradient descent runs much faster than batch gradient descent and that's pretty much what everyone in Deep Learning will use when you're training on a large data set. In the next video, let's delve deeper into mini-batch gradient descent so you can get a better understanding of what it is doing and why it works so well.\n\nUnderstanding Mini-batch Gradient Descent\nIn the previous video, you saw how you can use mini-batch gradient descent to start making progress and start taking gradient descent steps, even when you're just partway through processing your training set even for the first time. In this video, you learn more details of how to implement gradient descent and gain a better understanding of what it's doing and why it works. With batch gradient descent on every iteration you go through the entire training set and you'd expect the cost to go down on every single iteration.\nSo if we've had the cost function j as a function of different iterations it should decrease on every single iteration. And if it ever goes up even on iteration then something is wrong. Maybe you're running ways to big. On mini batch gradient descent though, if you plot progress on your cost function, then it may not decrease on every iteration. In particular, on every iteration you're processing some X{t}, Y{t} and so if you plot the cost function J{t}, which is computer using just X{t}, Y{t}. Then it's as if on every iteration you're training on a different training set or really training on a different mini batch. So you plot the cross function J, you're more likely to see something that looks like this. It should trend downwards, but it's also going to be a little bit noisier.\nSo if you plot J{t}, as you're training mini batch in descent it may be over multiple epochs, you might expect to see a curve like this. So it's okay if it doesn't go down on every derivation. But it should trend downwards, and the reason it'll be a little bit noisy is that, maybe X{1}, Y{1} is just the rows of easy mini batch so your cost might be a bit lower, but then maybe just by chance, X{2}, Y{2} is just a harder mini batch. Maybe you needed some mislabeled examples in it, in which case the cost will be a bit higher and so on. So that's why you get these oscillations as you plot the cost when you're running mini batch gradient descent. Now one of the parameters you need to choose is the size of your mini batch. So m was the training set size on one extreme, if the mini-batch size,\n= m, then you just end up with batch gradient descent.\nAlright, so in this extreme you would just have one mini-batch X{1}, Y{1}, and this mini-batch is equal to your entire training set. So setting a mini-batch size m just gives you batch gradient descent. The other extreme would be if your mini-batch size, Were = 1.\nThis gives you an algorithm called stochastic gradient descent.\nAnd here every example is its own mini-batch.\nSo what you do in this case is you look at the first mini-batch, so X{1}, Y{1}, but when your mini-batch size is one, this just has your first training example, and you take derivative to sense that your first training example. And then you next take a look at your second mini-batch, which is just your second training example, and take your gradient descent step with that, and then you do it with the third training example and so on looking at just one single training sample at the time.\nSo let's look at what these two extremes will do on optimizing this cost function. If these are the contours of the cost function you're trying to minimize so your minimum is there. Then batch gradient descent might start somewhere and be able to take relatively low noise, relatively large steps. And you could just keep matching to the minimum. In contrast with stochastic gradient descent If you start somewhere let's pick a different starting point. Then on every iteration you're taking gradient descent with just a single strain example so most of the time you hit two at the global minimum. But sometimes you hit in the wrong direction if that one example happens to point you in a bad direction. So stochastic gradient descent can be extremely noisy. And on average, it'll take you in a good direction, but sometimes it'll head in the wrong direction as well. As stochastic gradient descent won't ever converge, it'll always just kind of oscillate and wander around the region of the minimum. But it won't ever just head to the minimum and stay there. In practice, the mini-batch size you use will be somewhere in between.\nSomewhere between in 1 and m and 1 and m are respectively too small and too large. And here's why. If you use batch gradient descent, So this is your mini batch size equals m.\nThen you're processing a huge training set on every iteration. So the main disadvantage of this is that it takes too much time too long per iteration assuming you have a very long training set. If you have a small training set then batch gradient descent is fine. If you go to the opposite, if you use stochastic gradient descent,\nThen it's nice that you get to make progress after processing just tone example that's actually not a problem. And the noisiness can be ameliorated or can be reduced by just using a smaller learning rate. But a huge disadvantage to stochastic gradient descent is that you lose almost all your speed up from vectorization.\nBecause, here you're processing a single training example at a time. The way you process each example is going to be very inefficient. So what works best in practice is something in between where you have some,\nMini-batch size not to big or too small.\nAnd this gives you in practice the fastest learning.\nAnd you notice that this has two good things going for it. One is that you do get a lot of vectorization. So in the example we used on the previous video, if your mini batch size was 1000 examples then, you might be able to vectorize across 1000 examples which is going to be much faster than processing the examples one at a time.\nAnd second, you can also make progress,\nWithout needing to wait til you process the entire training set.\nSo again using the numbers we have from the previous video, each epoch each part your training set allows you to see 5,000 gradient descent steps.\nSo in practice they'll be some in-between mini-batch size that works best. And so with mini-batch gradient descent we'll start here, maybe one iteration does this, two iterations, three, four. And It's not guaranteed to always head toward the minimum but it tends to head more consistently in direction of the minimum than the consequent descent. And then it doesn't always exactly convert or oscillate in a very small region. If that's an issue you can always reduce the learning rate slowly. We'll talk more about learning rate decay or how to reduce the learning rate in a later video. So if the mini-batch size should not be m and should not be 1 but should be something in between, how do you go about choosing it? Well, here are some guidelines. First, if you have a small training set, Just use batch gradient descent.\nIf you have a small training set then no point using mini-batch gradient descent you can process a whole training set quite fast. So you might as well use batch gradient descent. What a small training set means, I would say if it's less than maybe 2000 it'd be perfectly fine to just use batch gradient descent. Otherwise, if you have a bigger training set, typical mini batch sizes would be,\nAnything from 64 up to maybe 512 are quite typical. And because of the way computer memory is layed out and accessed, sometimes your code runs faster if your mini-batch size is a power of 2. All right, so 64 is 2 to the 6th, is 2 to the 7th, 2 to the 8, 2 to the 9, so often I'll implement my mini-batch size to be a power of 2. I know that in a previous video I used a mini-batch size of 1000, if you really wanted to do that I would recommend you just use your 1024, which is 2 to the power of 10. And you do see mini batch sizes of size 1024, it is a bit more rare. This range of mini batch sizes, a little bit more common. One last tip is to make sure that your mini batch,\nAll of your X{t}, Y{t} that that fits in CPU/GPU memory.\nAnd this really depends on your application and how large a single training sample is. But if you ever process a mini-batch that doesn't actually fit in CPU, GPU memory, whether you're using the process, the data. Then you find that the performance suddenly falls of a cliff and is suddenly much worse. So I hope this gives you a sense of the typical range of mini batch sizes that people use. In practice of course the mini batch size is another hyper parameter that you might do a quick search over to try to figure out which one is most sufficient of reducing the cost function j. So what i would do is just try several different values. Try a few different powers of two and then see if you can pick one that makes your gradient descent optimization algorithm as efficient as possible. But hopefully this gives you a set of guidelines for how to get started with that hyper parameter search. You now know how to implement mini-batch gradient descent and make your algorithm run much faster, especially when you're training on a large training set. But it turns out there're even more efficient algorithms than gradient descent or mini-batch gradient descent. Let's start talking about them in the next few videos.\n\nExponentially Weighted Averages\nI want to show you a few optimization algorithms. They are faster than gradient descent. In order to understand those algorithms, you need to be able they use something called exponentially weighted averages. Also called exponentially weighted moving averages in statistics. Let's first talk about that, and then we'll use this to build up to more sophisticated optimization algorithms. So, even though I now live in the United States, I was born in London. So, for this example I got the daily temperature from London from last year. So, on January 1, temperature was 40 degrees Fahrenheit. Now, I know most of the world uses a Celsius system, but I guess I live in United States which uses Fahrenheit. So that's four degrees Celsius. And on January 2, it was nine degrees Celsius and so on. And then about halfway through the year, a year has 365 days so, that would be, sometime day number 180 will be sometime in late May, I guess. It was 60 degrees Fahrenheit which is 15 degrees Celsius, and so on. So, it start to get warmer, towards summer and it was colder in January. So, you plot the data you end up with this. Where day one being sometime in January, that you know, being the, beginning of summer, and that's the end of the year, kind of late December. So, this would be January, January 1, is the middle of the year approaching summer, and this would be the data from the end of the year. So, this data looks a little bit noisy and if you want to compute the trends, the local average or a moving average of the temperature, here's what you can do. Let's initialize V zero equals zero. And then, on every day, we're going to average it with a weight of 0.9 times whatever appears as value, plus 0.1 times that day temperature. So, theta one here would be the temperature from the first day. And on the second day, we're again going to take a weighted average. 0.9 times the previous value plus 0.1 times today's temperature and so on. Day two plus 0.1 times theta three and so on. And the more general formula is V on a given day is 0.9 times V from the previous day, plus 0.1 times the temperature of that day. So, if you compute this and plot it in red, this is what you get. You get a moving average of what's called an exponentially weighted average of the daily temperature. So, let's look at the equation we had from the previous slide, it was VT equals, previously we had 0.9. We'll now turn that to prime to beta, beta times VT minus one plus and it previously, was 0.1, I'm going to turn that into one minus beta times theta T, so, previously you had beta equals 0.9. It turns out that for reasons we are going to later, when you compute this you can think of VT as approximately averaging over, something like one over one minus beta, day's temperature. So, for example when beta goes 0.9 you could think of this as averaging over the last 10 days temperature. And that was the red line. Now, let's try something else. Let's set beta to be very close to one, let's say it's 0.98. Then, if you look at 1/1 minus 0.98, this is equal to 50. So, this is, you know, think of this as averaging over roughly, the last 50 days temperature. And if you plot that you get this green line. So, notice a couple of things with this very high value of beta. The plot you get is much smoother because you're now averaging over more days of temperature. So, the curve is just, you know, less wavy is now smoother, but on the flip side the curve has now shifted further to the right because you're now averaging over a much larger window of temperatures. And by averaging over a larger window, this formula, this exponentially weighted average formula. It adapts more slowly, when the temperature changes. So, there's just a bit more latency. And the reason for that is when Beta 0.98 then it's giving a lot of weight to the previous value and a much smaller weight just 0.02, to whatever you're seeing right now. So, when the temperature changes, when temperature goes up or down, there's exponentially weighted average. Just adapts more slowly when beta is so large. Now, let's try another value. If you set beta to another extreme, let's say it is 0.5, then this by the formula we have on the right. This is something like averaging over just two days temperature, and you plot that you get this yellow line. And by averaging only over two days temperature, you have a much, as if you're averaging over much shorter window. So, you're much more noisy, much more susceptible to outliers. But this adapts much more quickly to what the temperature changes. So, this formula is highly implemented, exponentially weighted average. Again, it's called an exponentially weighted, moving average in the statistics literature. We're going to call it exponentially weighted average for short and by varying this parameter or later we'll see such a hyper parameter if you're learning algorithm you can get slightly different effects and there will usually be some value in between that works best. That gives you the red curve which you know maybe looks like a beta average of the temperature than either the green or the yellow curve. You now know the basics of how to compute exponentially weighted averages. In the next video, let's get a bit more intuition about what it's doing.\n\nUnderstanding Exponentially Weighted Averages\nIn the last video, we talked about exponentially weighted averages. This will turn out to be a key component of several optimization algorithms that you used to train your neural networks. So, in this video, I want to delve a little bit deeper into intuitions for what this algorithm is really doing. Recall that this is a key equation for implementing exponentially weighted averages. And so, if beta equals 0.9 you got the red line. If it was much closer to one, if it was 0.98, you get the green line. And it it's much smaller, maybe 0.5, you get the yellow line. Let's look a bit more than that to understand how this is computing averages of the daily temperature. So here's that equation again, and let's set beta equals 0.9 and write out a few equations that this corresponds to. So whereas, when you're implementing it you have T going from zero to one, to two to three, increasing values of T. To analyze it, I've written it with decreasing values of T. And this goes on. So let's take this first equation here, and understand what V100 really is. So V100 is going to be, let me reverse these two terms, it's going to be 0.1 times theta 100, plus 0.9 times whatever the value was on the previous day. Now, but what is V99? Well, we'll just plug it in from this equation. So this is just going to be 0.1 times theta 99, and again I've reversed these two terms, plus 0.9 times V98. But then what is V98? Well, you just get that from here. So you can just plug in here, 0.1 times theta 98, plus 0.9 times V97, and so on. And if you multiply all of these terms out, you can show that V100 is 0.1 times theta 100 plus. Now, let's look at coefficient on theta 99, it's going to be 0.1 times 0.9, times theta 99. Now, let's look at the coefficient on theta 98, there's a 0.1 here times 0.9, times 0.9. So if we expand out the Algebra, this become 0.1 times 0.9 squared, times theta 98. And, if you keep expanding this out, you find that this becomes 0.1 times 0.9 cubed, theta 97 plus 0.1, times 0.9 to the fourth, times theta 96, plus dot dot dot. So this is really a way to sum and that's a weighted average of theta 100, which is the current days temperature and we're looking for a perspective of V100 which you calculate on the 100th day of the year. But those are sum of your theta 100, theta 99, theta 98, theta 97, theta 96, and so on. So one way to draw this in pictures would be if, let's say we have some number of days of temperature. So this is theta and this is T. So theta 100 will be sum value, then theta 99 will be sum value, theta 98, so these are, so this is T equals 100, 99, 98, and so on, ratio of sum number of days of temperature. And what we have is then an exponentially decaying function. So starting from 0.1 to 0.9, times 0.1 to 0.9 squared, times 0.1, to and so on. So you have this exponentially decaying function. And the way you compute V100, is you take the element wise product between these two functions and sum it up. So you take this value, theta 100 times 0.1, times this value of theta 99 times 0.1 times 0.9, that's the second term and so on. So it's really taking the daily temperature, multiply with this exponentially decaying function, and then summing it up. And this becomes your V100. It turns out that, up to details that are for later. But all of these coefficients, add up to one or add up to very close to one, up to a detail called bias correction which we'll talk about in the next video. But because of that, this really is an exponentially weighted average. And finally, you might wonder, how many days temperature is this averaging over. Well, it turns out that 0.9 to the power of 10, is about 0.35 and this turns out to be about one over E, one of the base of natural algorithms. And, more generally, if you have one minus epsilon, so in this example, epsilon would be 0.1, so if this was 0.9, then one minus epsilon to the one over epsilon. This is about one over E, this about 0.34, 0.35. And so, in other words, it takes about 10 days for the height of this to decay to around 1/3 already one over E of the peak. So it's because of this, that when beta equals 0.9, we say that, this is as if you're computing an exponentially weighted average that focuses on just the last 10 days temperature. Because it's after 10 days that the weight decays to less than about a third of the weight of the current day. Whereas, in contrast, if beta was equal to 0.98, then, well, what do you need 0.98 to the power of in order for this to really small? Turns out that 0.98 to the power of 50 will be approximately equal to one over E. So the way to be pretty big will be bigger than one over E for the first 50 days, and then they'll decay quite rapidly over that. So intuitively, this is the hard and fast thing, you can think of this as averaging over about 50 days temperature. Because, in this example, to use the notation here on the left, it's as if epsilon is equal to 0.02, so one over epsilon is 50. And this, by the way, is how we got the formula, that we're averaging over one over one minus beta or so days. Right here, epsilon replace a row of 1 minus beta. It tells you, up to some constant roughly how many days temperature you should think of this as averaging over. But this is just a rule of thumb for how to think about it, and it isn't a formal mathematical statement. Finally, let's talk about how you actually implement this. Recall that we start over V0 initialized as zero, then compute V one on the first day, V2, and so on. Now, to explain the algorithm, it was useful to write down V0, V1, V2, and so on as distinct variables. But if you're implementing this in practice, this is what you do: you initialize V to be called to zero, and then on day one, you would set V equals beta, times V, plus one minus beta, times theta one. And then on the next day, you add update V, to be called to beta V, plus 1 minus beta, theta 2, and so on. And some of it uses notation V subscript theta to denote that V is computing this exponentially weighted average of the parameter theta. So just to say this again but for a new format, you set V theta equals zero, and then, repeatedly, have one each day, you would get next theta T, and then set to V, theta gets updated as beta, times the old value of V theta, plus one minus beta, times the current value of V theta. So one of the advantages of this exponentially weighted average formula, is that it takes very little memory. You just need to keep just one row number in computer memory, and you keep on overwriting it with this formula based on the latest values that you got. And it's really this reason, the efficiency, it just takes up one line of code basically and just storage and memory for a single row number to compute this exponentially weighted average. It's really not the best way, not the most accurate way to compute an average. If you were to compute a moving window, where you explicitly sum over the last 10 days, the last 50 days temperature and just divide by 10 or divide by 50, that usually gives you a better estimate. But the disadvantage of that, of explicitly keeping all the temperatures around and sum of the last 10 days is it requires more memory, and it's just more complicated to implement and is computationally more expensive. So for things, we'll see some examples on the next few videos, where you need to compute averages of a lot of variables. This is a very efficient way to do so both from computation and memory efficiency point of view which is why it's used in a lot of machine learning. Not to mention that there's just one line of code which is, maybe, another advantage. So, now, you know how to implement exponentially weighted averages. There's one more technical detail that's worth for you knowing about called bias correction. Let's see that in the next video, and then after that, you will use this to build a better optimization algorithm than the straight forward create\n\nBias Correction in Exponentially Weighted Averages\nYou've learned how to implement exponentially weighted averages. There's one technical detail called bias correction that can make your computation of these averages more accurate. Let's see how that works. In the previous video, you saw this figure for Beta equals 0.9, this figure for a Beta equals 0.98. But it turns out that if you implement the formula as written here, you won't actually get the green curve when Beta equals 0.98, you actually get the purple curve here. You notice that the purple curve starts off really low. Let's see how to fix that. When implementing a moving average, you initialize it with V_0 equals 0, and then V_1 is equal to 0.98 V_0 plus 0.02 Theta 1. But V_0 is equal to 0, so that term just goes away. So V_1 is just 0.02 times Theta 1. That's why if the first day's temperature is, say, 40 degrees Fahrenheit, then V_1 will be 0.02 times 40, which is 0.8, so you get a much lower value down here. That's not a very good estimate of the first day's temperature. V_2 will be 0.98 times V_1 plus 0.02 times Theta 2. If you plug in V_1, which is this down here, and multiply it out, then you find that V_2 is actually equal to 0.98 times 0.02 times Theta 1 plus 0.02 times Theta 2 and that's 0.0196 Theta 1 plus 0.02 Theta 2. Assuming Theta 1 and Theta 2 are positive numbers. When you compute this, V_2 will be much less than Theta 1 or Theta 2, so V_2 isn't a very good estimate of the first two days temperature of the year. It turns out that there's a way to modify this estimate that makes it much better, that makes it more accurate, especially during this initial phase of your estimate. Instead of taking V_t, take V_t divided by 1 minus Beta to the power of t, where t is the current day that you're on. Let's take a concrete example. When t is equal to 2, 1 minus Beta to the power of t is 1 minus 0.98 squared. It turns out that is 0.0396. Your estimate of the temperature on day 2 becomes V_2 divided by 0.0396, and this is going to be 0.0196 times Theta 1 plus 0.02 Theta 2. You notice that these two things act as denominator, 0.0396. This becomes a weighted average of Theta 1 and Theta 2 and this removes this bias. You notice that as t becomes large, Beta to the t will approach 0, which is why when t is large enough, the bias correction makes almost no difference. This is why when t is large, the purple line and the green line pretty much overlap. But during this initial phase of learning, when you're still warming up your estimates, bias correction can help you obtain a better estimate of the temperature. This is bias correction that helps you go from the purple line to the green line. In machine learning, for most implementations of the exponentially weighted average, people don't often bother to implement bias corrections because most people would rather just weigh that initial period and have a slightly more biased assessment and then go from there. But we are concerned about the bias during this initial phase, while your exponentially weighted moving average is warming up, then bias correction can help you get a better estimate early on. With that, you now know how to implement exponentially weighted moving averages. Let's go on and use this to build some better optimization algorithms.\n\nGradient Descent with Momentum\nThere's an algorithm called momentum, or gradient descent with momentum that almost always works faster than the standard gradient descent algorithm. In one sentence, the basic idea is to compute an exponentially weighted average of your gradients, and then use that gradient to update your weights instead. In this video, let's unpack that one-sentence description and see how you can actually implement this. As a example let's say that you're trying to optimize a cost function which has contours like this. So the red dot denotes the position of the minimum. Maybe you start gradient descent here and if you take one iteration of gradient descent either or descent maybe end up heading there. But now you're on the other side of this ellipse, and if you take another step of gradient descent maybe you end up doing that. And then another step, another step, and so on. And you see that gradient descents will sort of take a lot of steps, right? Just slowly oscillate toward the minimum. And this up and down oscillations slows down gradient descent and prevents you from using a much larger learning rate. In particular, if you were to use a much larger learning rate you might end up over shooting and end up diverging like so. And so the need to prevent the oscillations from getting too big forces you to use a learning rate that's not itself too large. Another way of viewing this problem is that on the vertical axis you want your learning to be a bit slower, because you don't want those oscillations. But on the horizontal axis, you want faster learning.\nRight, because you want it to aggressively move from left to right, toward that minimum, toward that red dot. So here's what you can do if you implement gradient descent with momentum.\nOn each iteration, or more specifically, during iteration t you would compute the usual derivatives dw, db. I'll omit the superscript square bracket l's but you compute dw, db on the current mini-batch. And if you're using batch gradient descent, then the current mini-batch would be just your whole batch. And this works as well off a batch gradient descent. So if your current mini-batch is your entire training set, this works fine as well. And then what you do is you compute vdW to be Beta vdw plus 1 minus Beta dW. So this is similar to when we're previously computing the theta equals beta v theta plus 1 minus beta theta t.\nRight, so it's computing a moving average of the derivatives for w you're getting. And then you similarly compute vdb equals that plus 1 minus Beta times db. And then you would update your weights using W gets updated as W minus the learning rate times, instead of updating it with dW, with the derivative, you update it with vdW. And similarly, b gets updated as b minus alpha times vdb. So what this does is smooth out the steps of gradient descent.\nFor example, let's say that in the last few derivatives you computed were this, this, this, this, this.\nIf you average out these gradients, you find that the oscillations in the vertical direction will tend to average out to something closer to zero. So, in the vertical direction, where you want to slow things down, this will average out positive and negative numbers, so the average will be close to zero. Whereas, on the horizontal direction, all the derivatives are pointing to the right of the horizontal direction, so the average in the horizontal direction will still be pretty big. So that's why with this algorithm, with a few iterations you find that the gradient descent with momentum ends up eventually just taking steps that are much smaller oscillations in the vertical direction, but are more directed to just moving quickly in the horizontal direction. And so this allows your algorithm to take a more straightforward path, or to damp out the oscillations in this path to the minimum. One intuition for this momentum which works for some people, but not everyone is that if you're trying to minimize your bowl shape function, right? This is really the contours of a bowl. I guess I'm not very good at drawing. They kind of minimize this type of bowl shaped function then these derivative terms you can think of as providing acceleration to a ball that you're rolling down hill. And these momentum terms you can think of as representing the velocity.\nAnd so imagine that you have a bowl, and you take a ball and the derivative imparts acceleration to this little ball as the little ball is rolling down this hill, right? And so it rolls faster and faster, because of acceleration. And data, because this number a little bit less than one, displays a row of friction and it prevents your ball from speeding up without limit. But so rather than gradient descent, just taking every single step independently of all previous steps. Now, your little ball can roll downhill and gain momentum, but it can accelerate down this bowl and therefore gain momentum. I find that this ball rolling down a bowl analogy, it seems to work for some people who enjoy physics intuitions. But it doesn't work for everyone, so if this analogy of a ball rolling down the bowl doesn't work for you, don't worry about it. Finally, let's look at some details on how you implement this. Here's the algorithm and so you now have two\nhyperparameters of the learning rate alpha, as well as this parameter Beta, which controls your exponentially weighted average. The most common value for Beta is 0.9. We're averaging over the last ten days temperature. So it is averaging of the last ten iteration's gradients. And in practice, Beta equals 0.9 works very well. Feel free to try different values and do some hyperparameter search, but 0.9 appears to be a pretty robust value. Well, and how about bias correction, right? So do you want to take vdW and vdb and divide it by 1 minus beta to the t. In practice, people don't usually do this because after just ten iterations, your moving average will have warmed up and is no longer a bias estimate. So in practice, I don't really see people bothering with bias correction when implementing gradient descent or momentum. And of course, this process initialize the vdW equals 0. Note that this is a matrix of zeroes with the same dimension as dW, which has the same dimension as W. And Vdb is also initialized to a vector of zeroes. So, the same dimension as db, which in turn has same dimension as b. Finally, I just want to mention that if you read the literature on gradient descent with momentum often you see it with this term omitted, with this 1 minus Beta term omitted. So you end up with vdW equals Beta vdw plus dW. And the net effect of using this version in purple is that vdW ends up being scaled by a factor of 1 minus Beta, or really 1 over 1 minus Beta. And so when you're performing these gradient descent updates, alpha just needs to change by a corresponding value of 1 over 1 minus Beta. In practice, both of these will work just fine, it just affects what's the best value of the learning rate alpha. But I find that this particular formulation is a little less intuitive. Because one impact of this is that if you end up tuning the hyperparameter Beta, then this affects the scaling of vdW and vdb as well. And so you end up needing to retune the learning rate, alpha, as well, maybe. So I personally prefer the formulation that I have written here on the left, rather than leaving out the 1 minus Beta term. But, so I tend to use the formula on the left, the printed formula with the 1 minus Beta term. But both versions having Beta equal 0.9 is a common choice of hyperparameter. It's just at alpha the learning rate would need to be tuned differently for these two different versions. So that's it for gradient descent with momentum. This will almost always work better than the straightforward gradient descent algorithm without momentum. But there's still other things we could do to speed up your learning algorithm. Let's continue talking about these in the next couple videos.\n\nRMSprop\nYou've seen how using momentum can speed up gradient descent. There's another algorithm called RMSprop, which stands for root mean square prop, that can also speed up gradient descent. Let's see how it works. Recall our example from before, that if you implement gradient descent, you can end up with huge oscillations in the vertical direction, even while it's trying to make progress in the horizontal direction. In order to provide intuition for this example, let's say that the vertical axis is the parameter b and horizontal axis is the parameter w. It could be w1 and w2 where some of the center parameters was named as b and w for the sake of intuition. And so, you want to slow down the learning in the b direction, or in the vertical direction. And speed up learning, or at least not slow it down in the horizontal direction. So this is what the RMSprop algorithm does to accomplish this. On iteration t, it will compute as usual the derivative dW, db on the current mini-batch.\nSo I was going to keep this exponentially weighted average. Instead of VdW, I'm going to use the new notation SdW. So SdW is equal to beta times their previous value + 1- beta times dW squared. Sometimes write this dW star star 2, to deliniate expensation we will just write this as dw squared. So for clarity, this squaring operation is an element-wise squaring operation. So what this is doing is really keeping an exponentially weighted average of the squares of the derivatives. And similarly, we also have Sdb equals beta Sdb + 1- beta, db squared. And again, the squaring is an element-wise operation. Next, RMSprop then updates the parameters as follows. W gets updated as W minus the learning rate, and whereas previously we had alpha times dW, now it's dW divided by square root of SdW. And b gets updated as b minus the learning rate times, instead of just the gradient, this is also divided by, now divided by Sdb.\nSo let's gain some intuition about how this works. Recall that in the horizontal direction or in this example, in the W direction we want learning to go pretty fast. Whereas in the vertical direction or in this example in the b direction, we want to slow down all the oscillations into the vertical direction. So with this terms SdW an Sdb, what we're hoping is that SdW will be relatively small, so that here we're dividing by relatively small number. Whereas Sdb will be relatively large, so that here we're dividing yt relatively large number in order to slow down the updates on a vertical dimension. And indeed if you look at the derivatives, these derivatives are much larger in the vertical direction than in the horizontal direction. So the slope is very large in the b direction, right? So with derivatives like this, this is a very large db and a relatively small dw. Because the function is sloped much more steeply in the vertical direction than as in the b direction, than in the w direction, than in horizontal direction. And so, db squared will be relatively large. So Sdb will relatively large, whereas compared to that dW will be smaller, or dW squared will be smaller, and so SdW will be smaller. So the net effect of this is that your up days in the vertical direction are divided by a much larger number, and so that helps damp out the oscillations. Whereas the updates in the horizontal direction are divided by a smaller number. So the net impact of using RMSprop is that your updates will end up looking more like this.\nThat your updates in the, Vertical direction and then horizontal direction you can keep going. And one effect of this is also that you can therefore use a larger learning rate alpha, and get faster learning without diverging in the vertical direction. Now just for the sake of clarity, I've been calling the vertical and horizontal directions b and w, just to illustrate this. In practice, you're in a very high dimensional space of parameters, so maybe the vertical dimensions where you're trying to damp the oscillation is a sum set of parameters, w1, w2, w17. And the horizontal dimensions might be w3, w4 and so on, right?. And so, the separation there's a WMP is just an illustration. In practice, dW is a very high-dimensional parameter vector. Db is also very high-dimensional parameter vector, but your intuition is that in dimensions where you're getting these oscillations, you end up computing a larger sum. A weighted average for these squares and derivatives, and so you end up dumping ] out the directions in which there are these oscillations. So that's RMSprop, and it stands for root mean squared prop, because here you're squaring the derivatives, and then you take the square root here at the end. So finally, just a couple last details on this algorithm before we move on.\nIn the next video, we're actually going to combine RMSprop together with momentum. So rather than using the hyperparameter beta, which we had used for momentum, I'm going to call this hyperparameter beta 2 just to not clash. The same hyperparameter for both momentum and for RMSprop. And also to make sure that your algorithm doesn't divide by 0. What if square root of SdW, right, is very close to 0. Then things could blow up. Just to ensure numerical stability, when you implement this in practice you add a very, very small epsilon to the denominator. It doesn't really matter what epsilon is used. 10 to the -8 would be a reasonable default, but this just ensures slightly greater numerical stability that for numerical round off or whatever reason, that you don't end up dividing by a very, very small number. So that's RMSprop, and similar to momentum, has the effects of damping out the oscillations in gradient descent, in mini-batch gradient descent. And allowing you to maybe use a larger learning rate alpha. And certainly speeding up the learning speed of your algorithm. So now you know to implement RMSprop, and this will be another way for you to speed up your learning algorithm. One fun fact about RMSprop, it was actually first proposed not in an academic research paper, but in a Coursera course that Jeff Hinton had taught on Coursera many years ago. I guess Coursera wasn't intended to be a platform for dissemination of novel academic research, but it worked out pretty well in that case. And was really from the Coursera course that RMSprop started to become widely known and it really took off. We talked about momentum. We talked about RMSprop. It turns out that if you put them together you can get an even better optimization algorithm. Let's talk about that in the next video.\n\nAdam Optimization Algorithm\nDuring the history of deep learning, many researchers including some very well-known researchers, sometimes proposed optimization algorithms and show they work well in a few problems. But those optimization algorithms subsequently were shown not to really generalize that well to the wide range of neural networks you might want to train. Over time, I think the deep learning community actually developed some amount of skepticism about new optimization algorithms. A lot of people felt that gradient descent with momentum really works well, was difficult to propose things that work much better. RMSprop and the Adam optimization algorithm, which we'll talk about in this video, is one of those rare algorithms that has really stood up, and has been shown to work well across a wide range of deep learning architectures. This one of the algorithms that I wouldn't hesitate to recommend you try, because many people have tried it and seeing it work well on many problems. The Adam optimization algorithm is basically taking momentum and RMSprop, and putting them together. Let's see how that works. To implement Adam, you initialize V_dw equals 0, S_dw equals 0, and similarly V_db, S_db equals 0. Then on iteration t, you would compute the derivatives, compute dw, db using current mini-batch. Usually, you do this with mini-batch gradient descent, and then you do the momentum exponentially weighted average. V_dw equals Beta, but now I'm going to call this Beta_1 to distinguish it from the hyperparameter, Beta_2 we'll use for the RMSprop portion of this. This is exactly what we had when we're implementing momentum except they have now called the hyperparameter Beta _1 instead of Beta, and similarly you have V_db as follows, plus 1 minus Beta_1 times db, and then you do the RMSprop, like update as well. Now you have a different hyperparameter, Beta_2, plus 1, minus Beta_2 dw squared. Again, the squaring there, is element-wise squaring of your derivatives, dw. Then S_db is equal to this, plus 1 minus Beta_2, times db. This is the momentum-like update with hyperparameter Beta_1, and this is the RMSprop-like update with hyperparameter Beta_2. In the typical implementation of Adam, you do implement bias correction. You're going to have V corrected, corrected means after bias correction, dw equals V_dw, divided by 1 minus Beta_1 ^t, if you've done t elevations, and similarly, V_db corrected equals V_db divided by 1 minus Beta_1^t, and then similarly you implement this bias correction on S as well, so there's S_dw, divided by 1 minus Beta_2^t, and S_ db corrected equals S_db divided by 1 minus Beta_2^t. Finally, you perform the update. W gets updated as W minus Alpha times. If we're just implementing momentum, you'd use V_dw, or maybe V_dw corrected. But now we add in the RMSprop portion of this, so we're also going to divide by square root of S_dw corrected, plus Epsilon, and similarly, b gets updated as a similar formula. V_db corrected divided by square root S corrected, db plus Epsilon. These algorithm combines the effect of gradient descent with momentum together with gradient descent with RMSprop. This is commonly used learning algorithm that's proven to be very effective for many different neural networks of a very wide variety of architectures. This algorithm has a number of hyperparameters. The learning rate hyperparameter Alpha is still important, and usually needs to be tuned, so you just have to try a range of values and see what works. We did a default choice for Beta _1 is 0.9, so this is the weighted average of dw. This is the momentum-like term. The hyperparameter for Beta_2, the authors of the Adam paper inventors the Adam algorithm recommend 0.999. Again, this is computing the moving weighted average of dw squared as was db squared. The choice of Epsilon doesn't matter very much, but the authors of the Adam paper recommend a 10^minus 8, but this parameter, you really don't need to set it, and it doesn't affect performance much at all. But when implementing Adam, what people usually do is just use a default values of Beta_1 and Beta _2, as was Epsilon. I don't think anyone ever really tuned Epsilon, and then try a range of values of Alpha to see what works best. You can also tune Beta_1 and Beta_2, but is not done that often among the practitioners I know. Where does the term Adam come from? Adam stands for adaptive moment estimation, so Beta_1 is computing the mean of the derivatives. This is called the first moment, and Beta_2 is used to compute exponentially weighted average of the squares, and that's called the second moment. That gives rise to the name adaptive moment estimation. But everyone just calls it the Adam optimization algorithm. By the way, one of my long-term friends and collaborators is called Adam Coates. Far as I know, this algorithm doesn't have anything to do with him, except for the fact that I think he uses it sometimes, but sometimes I get asked that question. Just in case you're wondering. That's it for the Adam optimization algorithm. With it, I think you really train your neural networks much more quickly. But before we wrap up for this week, let's keep talking about hyperparameter tuning, as well as gain some more intuitions about what the optimization problem for neural networks looks like. In the next video, we'll talk about learning rate decay.\n\nLearning Rate Decay\nOne of the things that might help speed up your learning algorithm is to slowly reduce your learning rate over time. We call this learning rate decay. Let's see how you can implement this. Let's start with an example of why you might want to implement learning rate decay. Suppose you're implementing mini-batch gradient descents with a reasonably small mini-batch, maybe a mini-batch has just 64, 128 examples. Then as you iterate, your steps will be a little bit noisy and it will tend towards this minimum over here, but it won't exactly converge. But your algorithm might just end up wandering around and never really converge because you're using some fixed value for Alpha and there's just some noise in your different mini-batches. But if you were to slowly reduce your learning rate Alpha, then during the initial phases, while your learning rate Alpha is still large, you can still have relatively fast learning. But then as Alpha gets smaller, your steps you take will be slower and smaller, and so, you end up oscillating in a tighter region around this minimum rather than wandering far away even as training goes on and on. The intuition behind slowly reducing Alpha is that maybe during the initial steps of learning, you could afford to take much bigger steps, but then as learning approaches convergence, then having a slower learning rate allows you to take smaller steps. Here's how you can implement learning rate decay. Recall that one epoch is one pass through the data. If you have a training set as follows, maybe break it up into different mini-batches. Then the first pass through the training set is called the first epoch, and then the second pass is the second epoch, and so on. One thing you could do is set your learning rate Alpha to be equal to 1 over 1 plus a parameter, which I'm going to call the decay rate, times the epoch num. This is going to be times some initial learning rate Alpha 0. Note that the decay rate here becomes another hyperparameter which you might need to tune. Here's a concrete example. If you take several epochs, so several passes through your data, if Alpha 0 is equal to 0.2 and the decay rate is equal to 1, then during your first epoch, Alpha will be 1 over 1 plus 1 times Alpha 0, so your learning rate will be 0.1. That's just evaluating this formula when the decay rate is equal to 1 and epoch num is 1. On the second epoch, your learning rate decay is 0.67. On the third, 0.5. On the fourth, 0.4, and so on. Feel free to evaluate more of these values yourself and get a sense that as a function of epoch number, your learning rate gradually decreases, according to this formula up on top. If you wish to use learning rate decay, what you can do is try a variety of values of both hyperparameter Alpha 0, as well as this decay rate hyperparameter, and then try to find a value that works well. Other than this formula for learning rate decay, there are a few other ways that people use. For example, this is called exponential decay, where Alpha is equal to some number less than 1, such as 0.95, times epoch num times Alpha 0. This will exponentially quickly decay your learning rate. Other formulas that people use are things like Alpha equals some constant over epoch num square root times Alpha 0, or some constant k and another hyperparameter over the mini-batch number t square rooted times Alpha 0. Sometimes you also see people use a learning rate that decreases and discretes that, where for some number of steps, you have some learning rate, and then after a while, you decrease it by one-half, after a while, by one-half, after a while, by one-half, and so, this is a discrete staircase.\nSo far, we've talked about using some formula to govern how Alpha, the learning rate changes over time. One other thing that people sometimes do is manual decay. If you're training just one model at a time, and if your model takes many hours or even many days to train, what some people would do is just watch your model as it's training over a large number of days, and then now you say, oh, it looks like the learning rate slowed down, I'm going to decrease Alpha a little bit. Of course, this works, this manually controlling Alpha, really tuning Alpha by hand, hour-by-hour, day-by-day. This works only if you're training only a small number of models, but sometimes people do that as well. Now you have a few more options of how to control the learning rate Alpha. Now, in case you're thinking, wow, this is a lot of hyperparameters, how do I select amongst all these different options? I would say don't worry about it for now, and next week, we'll talk more about how to systematically choose hyperparameters. For me, I would say that learning rate decay is usually lower down on the list of things I try. Setting Alpha just a fixed value of Alpha and getting that to be well-tuned has a huge impact, learning rate decay does help. Sometimes it can really help speed up training, but it is a little bit lower down my list in terms of the things I would try. But next week, when we talk about hyperparameter tuning, you'll see more systematic ways to organize all of these hyperparameters and how to efficiently search amongst them. That's it for learning rate decay. Finally, I also want to talk a little bit about local optima and saddle points in neural networks so you can have a little bit better intuition about the types of optimization problems your optimization algorithm is trying to solve when you're trying to train these neural networks. Let's go onto the next video to see that.\n\nThe Problem of Local Optima\nIn the early days of deep learning, people used to worry a lot about the optimization algorithm getting stuck in bad local optima. But as this theory of deep learning has advanced, our understanding of local optima is also changing. Let me show you how we now think about local optima and problems in the optimization problem in deep learning. This was a picture people used to have in mind when they worried about local optima. Maybe you are trying to optimize some set of parameters, we call them W1 and W2, and the height in the surface is the cost function. In this picture, it looks like there are a lot of local optima in all those places. And it'd be easy for grading the sense, or one of the other algorithms to get stuck in a local optimum rather than find its way to a global optimum. It turns out that if you are plotting a figure like this in two dimensions, then it's easy to create plots like this with a lot of different local optima. And these very low dimensional plots used to guide their intuition. But this intuition isn't actually correct. It turns out if you create a neural network, most points of zero gradients are not local optima like points like this. Instead most points of zero gradient in a cost function are saddle points. So, that's a point where the zero gradient, again, just is maybe W1, W2, and the height is the value of the cost function J. But informally, a function of very high dimensional space, if the gradient is zero, then in each direction it can either be a convex light function or a concave light function. And if you are in, say, a 20,000 dimensional space, then for it to be a local optima, all 20,000 directions need to look like this. And so the chance of that happening is maybe very small, maybe two to the minus 20,000. Instead you're much more likely to get some directions where the curve bends up like so, as well as some directions where the curve function is bending down rather than have them all bend upwards. So that's why in very high-dimensional spaces you're actually much more likely to run into a saddle point like that shown on the right, then the local optimum. As for why the surface is called a saddle point, if you can picture, maybe this is a sort of saddle you put on a horse, right? Maybe this is a horse. This is a head of a horse, this is the eye of a horse. Well, not a good drawing of a horse but you get the idea. Then you, the rider, will sit here in the saddle. That's why this point here, where the derivative is zero, that point is called a saddle point. There's really the point on this saddle where you would sit, I guess, and that happens to have derivative zero. And so, one of the lessons we learned in history of deep learning is that a lot of our intuitions about low-dimensional spaces, like what you can plot on the left, they really don't transfer to the very high-dimensional spaces that any other algorithms are operating over. Because if you have 20,000 parameters, then J as your function over 20,000 dimensional vector, then you're much more likely to see saddle points than local optimum. If local optima aren't a problem, then what is a problem? It turns out that plateaus can really slow down learning and a plateau is a region where the derivative is close to zero for a long time. So if you're here, then gradient descents will move down the surface, and because the gradient is zero or near zero, the surface is quite flat. You can actually take a very long time, you know, to slowly find your way to maybe this point on the plateau. And then because of a random perturbation of left or right, maybe then finally I'm going to search pen colors for clarity. Your algorithm can then find its way off the plateau. Let it take this very long slope off before it's found its way here and they could get off this plateau. So the takeaways from this video are, first, you're actually pretty unlikely to get stuck in bad local optima so long as you're training a reasonably large neural network, save a lot of parameters, and the cost function J is defined over a relatively high dimensional space. But second, that plateaus are a problem and you can actually make learning pretty slow. And this is where algorithms like momentum or RmsProp or Adam can really help your learning algorithm as well. And these are scenarios where more sophisticated observation algorithms, such as Adam, can actually speed up the rate at which you could move down the plateau and then get off the plateau. So because your network is solving optimizations problems over such high dimensional spaces, to be honest, I don't think anyone has great intuitions about what these spaces really look like, and our understanding of them is still evolving. But I hope this gives you some better intuition about the challenges that the optimization algorithms may face. So that's congratulations on coming to the end of this week's content. Please take a look at this week's quiz as well as the exercise. I hope you enjoy practicing some of these ideas of this weeks exercise and I look forward to seeing you at the start of next week's videos.\n", "context_len": 13372, "questions": ["Question 1. Which of the following statements about Adam is False?\nA. We usually use \u201cdefault\u201d values for the hyperparameters \u03b21,\u03b22 and \u03b5 in Adam ( \u03b21 = 0.9 \u03b22 = 0.999, \u03b5=10\u22128)\nB. Adam should be used with batch gradient computations, not with mini-batches.\nC. The learning rate hyperparameter \u03b1 in Adam usually needs to be tuned.\nD. Adam combines the advantages of RMSProp and momentum", "Question 2. Suppose you have a deep learning model with 5 million parameters. Which of these techniques could help to reduce the memory requirements during training?\nA. Using mini-batch gradient descent instead of batch gradient descent\nB. Implementing dropout regularization\nC. Applying weight sharing\nD. Reducing the number of hidden layers", "Question 3. Which of the following statements is true about the initialization of weights in a deep neural network?\nA. Initializing all weights to zero is a good practice because it speeds up the convergence of the model.\nB. Initializing all weights to the same non-zero value is a good practice because it ensures symmetry in the model.\nC. Initializing weights to small random values is a good practice because it breaks the symmetry in the model.\nD. Initializing weights to large random values is a good practice because it speeds up the convergence of the model.", "Question 4. What is the main advantage of using a ReLU (rectified linear unit) activation function over a sigmoid activation function in deep neural networks?\nA. ReLU is computationally more efficient and helps to mitigate the vanishing gradient problem.\nB. ReLU provides a more complex decision boundary, leading to better performance.\nC. ReLU ensures that all neurons in the network are activated, increasing the model capacity.\nD. ReLU allows for better interpretability of the model's learned features.", "Question 5. In the context of deep learning, what is the purpose of using batch normalization?\nA. To improve the generalization performance and stabilize the training process.\nB. To speed up the training process by reducing the internal covariate shift.\nC. To simplify the architecture of the neural network.\nD. To increase the interpretability of the learned features.", "Question 6. What is the main advantage of using a dropout regularization technique in deep neural networks?\nA. It reduces the risk of overfitting by preventing complex co-adaptations between neurons.\nB. It speeds up the training process and provides more accurate results.\nC. It reduces the parameters of the neural network.\nD. It randomly set some values in a neural network to zero.", "Question 7. Which of the following assertions regarding mini-batch gradient descent do you concur with?\nA. Undertaking one epoch (a full pass through the training dataset) with mini-batch gradient descent is quicker than conducting one epoch with batch gradient descent.\nB. A single cycle of mini-batch gradient descent (performing calculations on one mini-batch) is faster than one cycle of batch gradient descent.\nC. You should set up mini-batch gradient descent without a direct for-loop over various mini-batches, thereby ensuring the algorithm handles all mini-batches simultaneously (vectorization).\nD. Mini-batch gradient descent always converges to the global minimum of the cost function, while batch gradient descent may get stuck in a local minimum.", "Question 8. Why is it important to shuffle the training data when using mini-batch gradient descent?\nA. Shuffling ensures that the model sees a diverse set of examples in each mini-batch, which can help the model generalize better.\nB. Shuffling speeds up the training process by reducing the risk of getting stuck in local optima.\nC. Shuffling reduces the risk of overfitting by preventing the model from memorizing the order of the training data.\nD. Shuffling improves the interpretability of the learned features.", "Question 9. Which of the following techniques can help to mitigate the vanishing gradient problem in deep neural networks?\nA. Using ReLU activation functions instead of sigmoid activation functions.\nB. Initializing weights to small random values.\nC. Implementing batch normalization.\nD. Applying dropout regularization.", "Question 10. In the context of deep learning, what is the purpose of using a learning rate decay schedule?\nA. To speed up the training process by allowing the model to take larger steps in the early stages of training.\nB. To improve the performance by forcing the model to pay attention to small features.\nC. To help the model converge more accurately by taking smaller steps in the later stages of training.\nD. To increase the scale of parameters in neural networks.", "Question 11. Which of the following factors can contribute to the vanishing gradient problem in deep neural networks?\nA. The choice of activation function.\nB. The depth of the network.\nC. The initialization of weights.\nD. The learning rate.", "Question 12. Assume that in a deep learning network, batch gradient descent is unusually slow in identifying a set of parameters that minimize the cost function J(W[1],b[1],\u2026,W[L],b[L]). Which strategies listed below could potentially help in achieving lower values for the cost function more quickly? (Select all relevant options)\nA. Implement mini-batch gradient descent\nB. Adjust the learning rate \u03b1\nC. Implement Adam optimization\nD. Improve the random weight initialization method"], "num_question": 12, "answer": ["B", "AC", "C", "A", "B", "A", "B", "A", "AC", "A", "ABC", "ABCD"], "dataset": "coursera"}